use std/flow::one
use std/flow::passBlock
use std/data/map::|entry
use std/ops/option::|wrap
use log/logger::Logger
use log/logger::stop as stopLogs
use log/log::logErrors
use log/log::logError
use log/log::logErrorMessage
use log/log::logInfoMessage
use distrib::DistributionEngine
use distrib::start
use distrib::stop
use work/distant::DistantEngine
use work/distant::distant
use work/resources/arch::Arch
use work/resources::Container
use work/resources::ServiceContainer
use work/resources::Volume

/**
Process dispatcher to spawn workers.

`api_url` and `api_token` are respectively the URI and job token to use to contact the API.  
`location` tells where job-worker should be spawned, might be one of those:
- `"api"` (default): ask the API to spawn required job-worker somewhere;
- `"compose"`: uses `podman compose` or `docker compose` (tried is this order) to spawn locally (or as the Podman/Docker configuration specifies) the job-worker, useful to test or run on local resources.
*/
model CicdDispatchEngine(location: string = "api", api_token: string, api_url: string = "https://api.melodium.tech/0.1"): DistantEngine {
    api_token = |wrap<string>(api_token)
    api_url = |wrap<string>(api_url)
    location = location
}

model CicdRunnerEngine(): DistributionEngine {
    treatment = "cicd/steps::step"
    version = "0.9.1"
}

/**
Setup a runner.

The runner configuration values are:
- `cpu`: CPU amount requested for the worker, in millicores (`1000` means one full CPU, `500` half of it);
- `memory`: memory requested for the worker, in megabytes;
- `storage`: filesystem storage requested for the worker, in megabytes;
- `max_duration`: maximum duration for which the worker will be effective, in seconds;

- `arch`: hardware architecture the worker must have (should be none if nothing specific is required);
- `edition`: Mélodium edition the worker must rely on (see on the Mélodium Services documentation to get the full list, can be none if nothing specific is required);

- `containers`: list of containers to instanciate alongside Mélodium engine as executors;
- `service_containers`: list of containers to instanciate alongside Mélodium engine as services;
- `volumes`: list of filesystem volumes that can be shared between the Mélodium engine and containers.

It should be noted that the CPU and memory requirements for the Mélodium engine and possible containers are cumulative.
Also, multiple different architecture cannot be requested for the same worker, so containers in the same request all have to use the same architecture.
Finally, the cumuled size of all volumes must be equal or less than the Mélodium engine storage value,
and each container must have storage values at least equal to the sum of the volumes mounted inside them.
*/
treatment setupRunner[dispatcher: CicdDispatchEngine, runner: CicdRunnerEngine, logger: Logger](
    name: string,
    cpu: u32,
    memory: u32,
    storage: u32,
    edition: Option<string> = _,
    arch: Option<Arch> = _,
    max_duration: u32 = 3600,
    volumes: Vec<Volume> = [],
    containers: Vec<Container> = [],
    service_containers: Vec<ServiceContainer> = [],
    var stop_on_failure: bool = true
  )
  input trigger: Block<void>
  output ready:  Block<void>
  output failed: Block<void>
{
    distant[distant_engine=dispatcher](cpu=cpu, memory=memory, storage=storage, edition=edition, arch=arch, max_duration=max_duration, volumes=volumes, containers=containers, service_containers=service_containers)

    logDistantErrors: logErrors[logger=logger](label=name)
    distant.errors -> logDistantErrors.messages

    start[distributor=runner](params=|entry<string>("label", name))
    logStartError: logError[logger=logger](label=name)
    start.error -> logStartError.message

    stopOnFailure: stop[distributor=runner]()
    oneFailed.value -> stopOnFailure.trigger

    logInfoMessage[logger=logger](label=name, message="Dispatch requested")
    Self.trigger -> logInfoMessage.trigger
    logErrorMessage[logger=logger](label=name, message="Dispatch failed")
    oneFailed.value -> logErrorMessage.trigger

    oneFailed: one<void>()
    distant.failed -> oneFailed.a
    start.failed ---> oneFailed.b

    Self.trigger -> distant.trigger,access -> start.access,ready -> Self.ready

    oneFailed.value -> Self.failed
}

treatment stopRunner[runner: CicdRunnerEngine]()
  input trigger: Block<void>
{
  stop[distributor=runner]()
  Self.trigger -> stop.trigger
}
